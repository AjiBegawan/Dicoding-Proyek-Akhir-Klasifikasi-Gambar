# -*- coding: utf-8 -*-
"""IndiartoAjiBegawan-ProyekAkhirKlasifikasiGambar

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ytnYfAK3gT3NzvlzGiwkPijm9dRC6dKJ

# Identitas Diri
Nama : Indiarto Aji Begawan \
Email : indiartoaji13@gmail.com \
Learning Path : Belajar Pengembangan Machine Learning \
Materi : Proyek Akhir - Klasifikasi Gambar \

# Library
"""

# from numba import cuda
# cuda.select_device(0)
# cuda.close()

# Commented out IPython magic to ensure Python compatibility.
import os
import zipfile,os
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

from google.colab import files
from tensorflow.keras import regularizers
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score

# %matplotlib inline

"""# Collect Dataset"""

!wget --no-check-certificate \
  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip \
  -O /tmp/rockpaperscissors.zip

local_zip = '/tmp/rockpaperscissors.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

base_path = '/tmp/rockpaperscissors/'
files_names = os.listdir(base_path)
files_names

!rm -r /tmp/rockpaperscissors/rps-cv-images

sum = 0
for i, names in enumerate(files_names):
  print(f"{names} : {len(os.listdir((base_path+names)))}")
  sum += len(os.listdir((base_path+names)))

print(f"\nSum of all images : {sum}")
num_classes = i+1
print(f"Number of classes : {num_classes}")

"""# Pre-Processing"""

train_dir = os.path.join(base_path)
train_datagen = ImageDataGenerator(rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode = 'nearest',
    brightness_range=[0.5,2.0],
    horizontal_flip=True,
    validation_split=0.4) # set validation split

size = (200,200)
batch = 16
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=size,
    batch_size=batch,
    class_mode='categorical',
    subset='training') # set as training data

validation_generator = train_datagen.flow_from_directory(
    train_dir, # same directory as training data
    target_size=size,
    batch_size=batch,
    class_mode='categorical',
    subset='validation')

"""# Training Model"""

reg = regularizers.l2(l=0.01)

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='valid', input_shape=(200,200, 3)),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='valid',kernel_regularizer=reg),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding='valid',kernel_regularizer=reg),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding='valid',kernel_regularizer=reg),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.4),  
    tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding='valid',kernel_regularizer=reg), 
    tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding='valid',kernel_regularizer=reg),  
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='valid',kernel_regularizer=reg),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='valid',kernel_regularizer=reg),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.4),  
    tf.keras.layers.Flatten(), 
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dense(num_classes, activation='softmax')  
])
model.summary()

# Callbacks
saved_path = "./Belajar Pengembangan Machine Learning/models/ProyekAkhir_KlasifikasiGambar.h5"
checkpoint = ModelCheckpoint(saved_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')
reducelr   = ReduceLROnPlateau(monitor='val_accuracy', factor=0, patience=100, min_delta=0.01,verbose=1)
cb_list    = [checkpoint, reducelr]

model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics = ['accuracy'])

history = model.fit(train_generator,
                    validation_data=validation_generator,
                    epochs=20,
                    verbose=1,
                    callbacks=cb_list)

# Grafik Accuracy dan Validation Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Grafik Accuracy dan Validation Accuracy
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

print("Please upload Paper-Rock-Scissor image")

uploaded = files.upload()

for fn in uploaded.keys():
 
  # predicting images
  path = fn
  img  = image.load_img(path, target_size=(200,200))

  imgplot = plt.imshow(img)
  x       = image.img_to_array(img)
  x       = np.expand_dims(x, axis=0)
  images  = np.vstack([x])

  classes = model.predict(images, batch_size=10)  
  predict = np.argmax(classes, axis=1)
  print(fn)

  if predict==0:
   print('Prediction : Paper')
  elif predict==1:
   print('Prediction : Rock')
  else:
    print('Prediction : Scissor')

validation_generator.reset()
Y_pred = model.predict(validation_generator, batch_size=batch)
y_pred = np.argmax(Y_pred, axis=1)

cm           = confusion_matrix(validation_generator.classes, y_pred)
cm_sum       = np.sum(cm, axis=1)
cm_perc      = cm / cm_sum.astype(float) * 100
annot        = np.empty_like(cm).astype(str)
nrows, ncols = cm.shape
for i in range(nrows):
  for j in range(ncols):
    c = cm[i, j]
    p = cm_perc[i, j]
    if i == j:
      s = cm_sum[i]
      annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
    elif c == 0:
      annot[i, j] = ''
    else:
      annot[i, j] = '%.1f%%\n%d' % (p, c)

figsize = (9,9)
fig, ax = plt.subplots(figsize=figsize)

sns.heatmap(cm, annot=annot, fmt='', ax=ax, cmap='Blues')
ax.set_title('Confusion Matrix dari Proyek Akhir - Klasifikasi Gambar\n');
ax.set_xlabel('Predicted Values')
ax.set_ylabel('Actual Values ');
ax.xaxis.set_ticklabels(['Paper', 'Rock', 'Scissor'])
ax.yaxis.set_ticklabels(['Paper', 'Rock', 'Scissor'])

print('Classification Report')
target_names = ['Paper', 'Rock', 'Scissor']
print(classification_report(validation_generator.classes, y_pred, target_names=target_names))

# set plot figure size
fig, c_ax = plt.subplots(1,1, figsize = (12, 8))

def multiclass_roc_auc_score(y_test, y_pred, average="macro"):
    lb = LabelBinarizer()
    lb.fit(y_test)
    y_test = lb.transform(y_test)
    y_pred = lb.transform(y_pred)

    all_labels = ['Paper', 'Rock', 'Scissor']
    for (idx, c_label) in enumerate(all_labels): # all_labels: no of the labels, for ex. ['cat', 'dog', 'rat']
        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])
        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))
    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')
    return roc_auc_score(y_test, y_pred, average=average)

# calling
validation_generator.reset() # resetting generator
y_pred = model.predict(validation_generator, verbose = True)
y_pred = np.argmax(y_pred, axis=1)
multiclass_roc_auc_score(validation_generator.classes, y_pred)